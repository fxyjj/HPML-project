{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules from keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# creates a generic neural network architecture\n",
    "model = Sequential()\n",
    "\n",
    "# hidden layer takes a pre-processed frame as input, and has 200 units\n",
    "model.add(Dense(units=200,input_dim=80*80, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(units=1, activation='sigmoid', kernel_initializer='RandomNormal'))\n",
    "\n",
    "# compile the model using traditional Machine Learning losses and optimizers\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# gym initialization\n",
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_input = None\n",
    "\n",
    "# Macros\n",
    "UP_ACTION = 2\n",
    "DOWN_ACTION = 3\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "\n",
    "# initialization of variables used in the main loop\n",
    "x_train, y_train, rewards = [],[],[]\n",
    "reward_sum = 0\n",
    "episode_nb = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easy_tf_log import tflog\n",
    "from datetime import datetime\n",
    "from keras import callbacks\n",
    "import os\n",
    "\n",
    "# initialize variables\n",
    "resume = True\n",
    "running_reward = None\n",
    "epochs_before_saving = 10\n",
    "log_dir = './log' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "\n",
    "# load pre-trained model if exist\n",
    "if (resume and os.path.isfile('my_model_weights.h5')):\n",
    "    print(\"loading previous weights\")\n",
    "    model.load_weights('my_model_weights.h5')\n",
    "    \n",
    "# add a callback tensorboard object to visualize learning\n",
    "tbCallBack = callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0,  \n",
    "          write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karpathy import prepro, discount_rewards\n",
    "\n",
    "# main loop\n",
    "while (True):\n",
    "\n",
    "    # preprocess the observation, set input as difference between images\n",
    "    cur_input = prepro(observation)\n",
    "    x = cur_input - prev_input if prev_input is not None else np.zeros(80 * 80)\n",
    "    prev_input = cur_input\n",
    "    \n",
    "    # forward the policy network and sample action according to the proba distribution\n",
    "    proba = model.predict(np.expand_dims(x, axis=1).T)\n",
    "    action = UP_ACTION if np.random.uniform() < proba else DOWN_ACTION\n",
    "    y = 1 if action == 2 else 0 # 0 and 1 are our labels\n",
    "\n",
    "    # log the input and label to train later\n",
    "    x_train.append(x)\n",
    "    y_train.append(y)\n",
    "\n",
    "    # do one step in our environment\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    # end of an episode\n",
    "    if done:\n",
    "        print('At the end of episode', episode_nb, 'the total reward was :', reward_sum)\n",
    "        \n",
    "        # increment episode number\n",
    "        episode_nb += 1\n",
    "        \n",
    "        # training\n",
    "        model.fit(x=np.vstack(x_train), y=np.vstack(y_train), verbose=1, callbacks=[tbCallBack], sample_weight=discount_rewards(rewards, gamma))\n",
    "        \n",
    "        # Saving the weights used by our model\n",
    "        if episode_nb % epochs_before_saving == 0:    \n",
    "            model.save_weights('my_model_weights' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.h5')\n",
    "        \n",
    "        # Log the reward\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        tflog('running_reward', running_reward, custom_dir=log_dir)\n",
    "        \n",
    "        # Reinitialization\n",
    "        x_train, y_train, rewards = [],[],[]\n",
    "        observation = env.reset()\n",
    "        reward_sum = 0\n",
    "        prev_input = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
